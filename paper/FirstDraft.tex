
\documentclass[12pt,journal,compsoc]{IEEEtran}
\ifCLASSOPTIONcompsoc
  % IEEE Computer Society needs nocompress option
  % requires cite.sty v4.0 or later (November 2003)
  % \usepackage[nocompress]{cite}
\else
  % normal IEEE
  % \usepackage{cite}
\fi

% *** GRAPHICS RELATED PACKAGES ***
%
%\usepackage{todonotes}
\usepackage{float}
\usepackage[font=small,labelfont=bf]{caption}
\renewcommand{\figurename}{Figure}
\renewcommand{\tablename}{Table}

\ifCLASSINFOpdf
  \usepackage[pdftex]{graphicx}

\else

\fi

\newcommand\MYhyperrefoptions{bookmarks=true,bookmarksnumbered=true,
pdfpagemode={UseOutlines},plainpages=false,pdfpagelabels=true,
colorlinks=true,linkcolor={black},citecolor={black},urlcolor={black},
pdftitle={Memory Constraints and Solutions for Heterogeneous SoC},%<!CHANGE!
pdfsubject={Typesetting},%<!CHANGE!
pdfauthor={Michael D. Shell},%<!CHANGE!
pdfkeywords={Computer Society, IEEEtran, journal, LaTeX, paper,
             template}}%<^!CHANGE!

% correct bad hyphenation here
\hyphenation{op-tical net-works semi-conduc-tor}


%\usepackage{helvet}
\usepackage{times}

\begin{document}
%
% paper title
% can use linebreaks \\ within to get better formatting as desired
% Do not put math or special symbols in the title.

%\title{Memory Constraints and Solutions \\ for Heterogeneous SoC}
 \title{\fontfamily{bull}\selectfont{\Huge{\textbf{Memory Constraints and Solutions \\ for Heterogeneous SoC}}}}
\author{\fontfamily{campria}\selectfont{Tudor Voicu, Saevar Hilmarsson and Misael Hernandez}}

% The paper headers
\markboth{}%
{Shell \MakeLowercase{\textit{et al.}}: Bare Advanced Demo of IEEEtran.cls for Journals}
% The only time the second header will appear is for the odd numbered pages

\maketitle

\IEEEdisplaynontitleabstractindextext

\IEEEpeerreviewmaketitle
\begin{abstract}
	Something to do about this shitty abstraaaaaaaact. Almost there, come on, Alex Hernandez, change me!!!
\end{abstract}
\section{Introduction}

\indent Recent developments start to show a modest but solid deviation to the well-known Gordon Moore's 40-year old law that talks about performance improvement in CPU processing power. Even though the mentioned law got several interpretations, only one seems to be able to keep up with the projected trend: the increase in number of transistor per die area (expected to double every 18-24 months). However, as we approach the limits of current Silicon manufacturing technologies, even that trend is threatened to flatten, not only because of the pure physical constraints (we are reaching manufacturing precisions of atoms), but also because of the more significant quantum phenomena that starts to be exhibited when talking about devices like SET (single-electron transistors).

One of the most important aspects left aside in the childhood of computing ages was the memory-barrier, probably the biggest constraint a computing system faces nowadays. There is a well known chart that addresses this issue, which shows that the memory-CPU gap also grows at an exponential rate. In other terms, this issue has a massive impact in CPU performance and further threatens to clutter the horizon of exponential innovation for the field of artificial intelligence and computing systems.

The complexity of the issue grows further with recent developments on heterogeneous SoC, because of integrating a bandwidth-hungry multi-core GPU on the same die. Although a higher level of integration can provide improvements in many areas, such as portability, power-efficiency and cost-efficiency, the bandwidth and latency challenges still remain and can truly bottleneck one of the two main components, regarding the provided scenario.

\section{Problems}
The main part of a conventional computing system architecture is comprised of a computing unit, it's physical memory, virtual memory and the data transfer buses between all of them. In order to keep up with the evolving state of processing power, memory hierarchy started to become more complex. Nowadays, SoCs include up to three levels of Cache memory, built as SRAM memory. The cache is tightly coupled with the compute units and is usually structured in two or three levels (L1, L2 and L3). The lowest level is the closest to the cores and thus should have the lowest latency possible. However the next levels which are less latency dependent can be shared between cores as a way to reduce hardware overhead. The term LLC refers to the last level cache, which in modern architectures it is commonly shared amongst all of the cores.

As of 2009, for means of tighter integration and power consumption, consumer CPUs have started to engulf some GPU processing cores. The solution brought many innovations in the field, but also gave birth to several challenges to computer engineers and software architects. 

The biggest problems of such a high-level of integration are the memory management, bandwidth and latency. The issue grows further in complexity because the memory requirements do not scale in a linear manner, but more agressively due to the combined needs the two processors exhibit (the GPU and CPU); while the conventional main-purpose processor needs low-latency memory and moderate bandwidth, the GPU is not affected by a high latency, while the bandwidth requirements can be an order of magnitude bigger.

One of the issues related to memory management in CPU-GPU solutions that is currently most affecting performance is the memory bandwidth. With CPU only solutions, this was already a problem. Engineers in charge of improving DRAM technology, which is used as main memory for many processors, have had to come up with ingenious solutions to the bandwidth issue. Sometimes increasing the burst length, other times rearranging the banks, memory technology has consistently been the weakest link. CPU-GPU processors now represent a new challenge: sharing the limited memory bandwidth with some bandwidth-hungry processing units.

GPUs support a considerably larger amount of threads. Coupled with the fact that many GPU applications are cache insensitive, meaning that there is almost zero reuse of cache lines, the GPU can significantly interfere with the CPU memory accesses. This can happen up to the point where CPU cores suffer from starvation, causing a heavy drop on the performance, measured as the IPC.

State of the art memory controllers (MC) have mainly been focused on a multi-core CPUs and not on the joint venture of CPU-GPU SoCs.  To make the applications-aware memory scheduling work on GPU-CPU SoC a very large request buffer is needed.
 
Figure~\ref{fig:buff}(a) shows the request buffer of a memory controller where there is only a multi-core CPU.  The MC has a good overview over numerous requests from the CPU cores and inside into their memory behavior.  When the buffer is shared between the GPU and CPU, as can be seen in Figure~\ref{fig:buff}(b) the requests from the GPU limit the MC capability of insight into the behavior of requests from the CPU cores.  Figure~\ref{fig:buff}(c) shows larger request buffer so that the MC can observer more requests from the CPU\cite{SmS}.

\begin{figure}[H]
	\centering
	\includegraphics[width = 8 cm]{graphics/no1.png}
	\caption{Limited visibility example due to small buffer size. (a) CPU-only
		case, (b) MC’s visibility when CPU and GPU are combined, (c) Improved
		visibility with a larger request buffer.\cite{SmS}}\label{fig:buff}
\end{figure} 

The increased buffer size does not come without a cost. It increases circuit complexity and results in a significant larger die area. The larger request buffer also leads to higher power consumption and increased costs \cite{SmS} making it an unattractive solution. 

Another possible solution for this issue would be sharing the LLC between the CPUs and the GPUs. However, most cache replacement policies were designed for homogeneous configurations, so they are not efficient in these cases.

Policies like LRU and DDRIP will favor the GPUs when allocating cache lines due to the fact that their memory access rates are orders of magnitude higher than the CPUs. GPU applications tend to be more latency tolerant while CPU ones are more latency intolerant. So favoring GPUs in cache allocation causes a considerable decrease in the CPUs IPC, while the GPUs IPC is only marginally improved, meaning that this is also far from an ideal solution.

Diving deeper into the DRAM, bandwidth is not the only limitation. The emerging market of mobile devices uses tightly integrated hardware for addressing the consumer's needs in terms of portability, processing power and energy consumption. The trend for smartphones and tablets has only one direction: slimmer and lighter devices. This trend forces a shrink on every subcomponent or a greater efficiency. While battery needs also to shrink (the LiPo capacity has a very low improvement rate), the energy consumption has to decrease without significantly influencing the processing improvement trend. 

Strictly speaking, a device's main memory needs to have greater capacity in smaller packages with lower energy consumption. Although the improvement in bandwidth is projected for another decade (LPDDR4 and its descendants), the power and area constraints push more and more against this technology.

\section{Solutions}
\subsection{Cache management in shared LLC}
The proposed solution is a new cache management policy called HeLM (Heterogeneous LLC Management). It consists on measuring the cache sensitivity of the GPU and CPU, and based on a couple of thresholds decide if GPU access requests to the LLC should be bypassed directly to the main memory or not.
It is based on the fact that GPUs have a much higher TLP than CPUs, and thus are less affected by longer memory access latency. Having a high TLP means that there will most likely be some threads that can be ran while waiting for the memory to respond. If that is the case, bypassing the cache should not affect the IPC count.

To calculate cache sensitivity in the GPUs, set dueling is used. The GPUs TLP is measured as the amount of wavefronts ready to be scheduled. This information is sent along with the LLC access request whenever there is an L1 cache miss. One GPU core is set to do a cache bypass whenever it's TLP is higher than a lowThreshold. Another one is set to do the same but with highThreshold. IPC is measured for both cores and if the difference is higher than pThreshold, the highThreshold is used for the remaining cores.

In the case of the CPUs, the cache misses on the LLC are measured with both lowThreshold and highThreshold bypassing of the GPUs. If the difference in cache misses is higher than mThreshold, the threshold of the GPUs is overridden to be the lowThreshold.

What all this means is that the CPU will be given preference over the GPU in cache allocation. The GPU will be frequently bypassing the LLC cache, specially if its TLP is high. So a penalty in the GPUs IPC is expected. However the CPU IPC is expected to see the most change, since the GPU bypassing of the LLC means the CPU will get more sets for its use. Figure~\ref{fig:algo} shows the algorithm being followed to determine the threshold used for LLC bypassing. The first comparison determines if there are too many cache misses for the CPU requests and if so sets the threshold to lowThreshold. This is how priority is given to the CPU requests.

\begin{figure}
	\centering
	\includegraphics[width = 6 cm]{graphics/LLCalgo.png}
	\caption{Algorithm followed to select the proper bypass threshold\cite{LLC}}\label{fig:algo}
\end{figure} 

\subsection{Hybrid-technology memory system}
Regarding the issue with the scaling of DRAM technology, the authors in \cite{PDRAM} and \cite{PRAM} proposed a hybrid DRAM/PRAM-based Main Memory. PRAM is a type of non-volatile RAM which stores information as phase-state in a cell. The PRAM’s main advantages over DRAM  are the better-scaling projections, the low idle-power consumption and the property of non-volatility. However, because of the low writing endurance and high-latency for writes (more detailed characteristics of the PRAM technology are also described in \cite{PDRAM} and \cite{PRAM}), the technology is not a valid replacement for DRAM, being best suited to complement it in the critical points (storage and power consumption).

Combining the benefits of the two technologies, the authors proposed two types of the hybrid system that can hide their weak points. 
\begin{figure}
	\centering
	\includegraphics[width = 8 cm]{graphics/structure-hybrid.png}
	\caption{Hybrid structure of PRAM/DRAM main-memory system: (a) Flat structure (b) Hierarchical structure \cite{PRAM}}\label{fig:structure}
\end{figure} 
First variant is a hierarchical memory organization, where the DRAM plays the role of a Last-Level Cache and the PRAM is the main memory. However, when regarding the CPU’s needs, a LLC miss would involve a bigger latency (98 ns for the PRAM against 45 ns for DRAM) for fetching the needed data from the main memory. Moreover, while the write-through policy would be impossible to apply (very high write cycle of PRAM - around 40 $\mu s$), the write-back would also stall the CPU when data bursts are sent to the LLC and fill it up. Therefore, without a significant amount of memory size for the LLC, the hierarchical organization of the hybrid memory system can significantly decrease the IPC (for a 1 GHz CPU a wait on writing to the main memory would take approximatively 40.000 cycles).

The second approach is a flat organization of the two technologies, with the DRAM mainly serving the CPU purposes, and the PRAM meant for the GPU side. This kind of implementation has several advantages over the hierarchical:
\begin{itemize}
\item{It eliminates the issue of a bigger miss penalty on the LLC (DRAM for that proposal).}
\item{It separates the memory-buses, avoiding possible GPU-CPU conflicts in critical points, such as the synchronization of complex CPU applications with memory-hungry GPU activity.}
\end{itemize}
\begin{figure}[H]
	\centering
	\includegraphics[width = 8 cm]{graphics/StructureWithBuffer.PNG}
	\caption{Overall view of the proposed hybrid structure with hot-write DRAM buffer \cite{PRAM}}\label{fig:structurebuff}
\end{figure} 
However, having the GPU working only with the PRAM memory can result in missing deadlines (because of the high write-latency), when write-intensive applications are under execution. Therefore, a dynamic hot-write buffer has been created in the DRAM memory to handle successive write requests to GPU main memory. 
\begin{figure}[H]
	\centering
	\includegraphics[width = 9 cm]{graphics/hot-write.PNG}
	\caption{Write address list used to keep track of frequency of PRAM memory usage \cite{PRAM}}\label{fig:hotwrite}
\end{figure} 
The in-DRAM write buffer should store important and frequently accessed data from the GPU. For this purpose, a hot write data management policy which identifies write-intensive data has been implemented. Therefore, a write address list will be used to store the memory entries (as presented in Figure \ref{fig:hotwrite}), with original addresses residing in the PRAM. The mentioned list is divided in two: a cold and a hot region. When a new GPU memory address is accessed, it gets inserted in the list at the beginning of the hot region. The entries already in the list are upgraded/downgraded one rank if they are used/superseded in use by another memory location (each entry consists of 64 B). If a list item falls out of the list , it gets evicted from the in-DRAM buffer and written to the PRAM.

Where CPU performance bound is important, the two mechanisms presented in \cite{PRAM} can also be implemented for carefully controlling the impact in CPU IPC: 
\begin{itemize}
	\item
prioritizing the CPU access to DRAM over the GPU with a FR-FCFS policy.
	\item
dynamically adjusting the in-DRAM write-buffer size for keeping the CPU IPC in a specified value range.
\end{itemize}

Another advantage provided by the hot-write buffer is the possibility of cell wear leveling by reducing the number of writes to a single bit (also by implementing the differential writing mechanism \cite{PRAM}), thus expanding the PRAM's lifetime and reliability.

\subsection{Staged memory scheduling}
The others in \cite{SmS} propose to solve the request buffer issue with a new memory controller (MC), Staged Memory Scheduling (SMS), that is a simpler implementation in hardware and provides performance and fairness improvements over the application-aware MC. It is split into three stages, the batch formation, batch scheduler and a DRAM command scheduler.

\begin{figure}[H]
	\centering
	\includegraphics[width = 8 cm]{graphics/SmS.png}
	\caption{Organization of the SMS \cite{SmS}}
\end{figure}

The batch formation is focused on row-buffer locality.  It groups together requests from applications that want to access the same row in the main memory. The second stage, the batch scheduler, schedules batches from different applications and the third stage, the DRAM Command Scheduler schedules requests while satisfying all DRAM constrains \cite{SmS}

Stage 1: Is built from simple FIFO structures that have one structure per sources.  One source is a single CPU-core or the GPU.  One batch is one or more memory requests from the same source that request access to the same DRAM row.  A batch is only ready when a request to a new DRAM row comes in, it is full or when the oldest request in the batch has exceeded a predefined age threshold.  A batch that is considered ready moves to stage 2 the Batch scheduler.

Stage 2:  The Batch Scheduler operates in two states, the pick  and the  drain.  In the pick state in picks a batch from stage 1 and than enters drain state where it feeds the FIFO batch to the DRAM Command Scheduler one request per clock cycle until the whole batch is gone.  The picking is done in one of two fashions.  It picks the batch using either round-robin policy or shortest job first (SJF).   SJF tends to favor latency-sensitive applications that tend to have fewer total requests and thus favoring the CPU over the GPU.  On the other hand the round-robin simply picks the next ready batch in a round-robin manner and thus servicing high memory-intensive applications and favoring the GPU over the CPU.  The batch scheduler uses SJF with a probability of $p$ and round-robin  $ 1 - p $ which will be explained better later.

Stage 3: DRAM Command Scheduler (DCS) consists of few FIFO queues or one per DRAM bank.  Since the row-buffer locality from stage 1 has been preserved the only job of the DCS is to issue low-level DRAM commands and ensuring DRAM protocol compliance. 

Some batches bypass stage 1 and 2 and go directly to the DCS. Latency-sensitive requests of applications with low memory-intensity and all requests wen the system is lightly loaded, that is when total number of in-flight requests over all sources in the DCS are fewer the 16.\\

The probability parameter $p$:  As stated above the probability of SJF used is $p$ and that of round-robin policy $1-p$.  Therefor the value of $p$ determines  which gets higher priority, the CPU or the
GPU.  That is when $p$ is high the SJF policy is performed more frequently then round-robin and therefore favoring less memory-intensive applications of the CPU over the GPU.  And on the other hand when p is low request batches are scheduled more often in round-robin fashion favoring memory-intensive applications of the GPU over the CPU.  As different systems have different performance needs it is proposed to have $p$ a dynamic parameter that can be tuned to the needs of different systems\cite{SmS}.\\

\textbf{Hardware Implementation}\\
Batch Formation:  The hardware implementation for stage 1 can be of a fairly simple nature as it only needs one comparator per source.  A new request has only to be checked if it can fit in the existing batch or not.  That is if the incoming request row index is the same as the last request that entered the batch.  The hardware overhead can be seen in Table \ref{tab:SmS1}

\begin{table}[H]
  \centering
  \resizebox{14.3 cm}{!}{\begin{minipage}{\textwidth}
	\begin{tabular}{| l | l |}
		\hline
		Storgae & Size \\ \hline
		CPU FIFO queues 			& $N_{core} × Queue\_Size_{core} = 160 entries$ 					\\\hline
		GPU FIFO queues 			& $N_{GPU} × Queue\_Size_{GPU} = 20 entries$ 						\\\hline
		MPKC counters   			& $N_{core} × log_2 MPKC_{max} = 160 bits$							\\\hline
		Last request’s row index	& $(N_{core} + N_{GPU} ) × log_2\_ Row\_ Index\_ Size = 204 bits$ 	\\\hline
	\end{tabular}
	
  \end{minipage} }
  \caption{Storage Overhead of Stage 1: Batch formation stage \cite{SmS}}\label{tab:SmS1}
\end{table}

Batch Scheduler:  The batch scheduler does not need to be very complex as it only schedules FIFOs that are as many as there are sources or cores.  When SJF is applied a tree of MIN operators can be build from the out standing batches.  As the batches are not many the tree will not get very large. The hardware overhead can be seen in Table \ref{tab:SmS2}

\begin{table}[H]
  \centering
  \resizebox{15.5 cm}{!}{\begin{minipage}{\textwidth}
	\begin{tabular}{| l | l |}
		\hline
		Storgae & Size \\ \hline
		CPU memory request counters 			& $N_{core} × log_2 Count_{max\_CPU} = 80 bits$ 					\\\hline
		GPU memory request counter 			& $N_{GPU} × log_2 Count_{max\_GPU} = 10 bits$ 						\\\hline	
	\end{tabular}
  \end{minipage} }
  \caption{Storage Overhead of Stage 2: Batch Schedule \cite{SmS}}\label{tab:SmS2}
\end{table}

DRAM Command Scheduler:   Compared to the monolithic scheduler the DRAM command scheduler is on a completely different scale.  It consist of per-bank FIFOs, eight per rank for DDR3, and only needs to look at the head of one. The hardware overhead can be seen in Table \ref{tab:SmS3}

\begin{table}[H]
  \centering
  \resizebox{17 cm}{!}{\begin{minipage}{\textwidth}
	\begin{tabular}{| l | l |}
		\hline
		Storgae & Size \\ \hline
		Per-Bank FIFO queues			& $N_{banks} × Queue\_Size_{bank} = 120 entries$ 					\\\hline
	\end{tabular}
  \end{minipage} }
  \caption{Storage Overhead of Stage 3: DRAM Command Scheduler \cite{SmS}\label{tab:SmS3}}
\end{table}

\subsection{Dynamic Quality of Service}
Another memory-resource management approach is the Dynamic Quality of Service mechanism. As described in paper \cite{QoS}, this solution offers an advantage over the static version, by balancing the priority in bandwidth allocation between the CPU and the GPU, for means of keeping an acceptable predefined level of rendering quality for the integrated graphics card. 
\begin{equation}
\small{FrameProgress = \frac{Number\, of\, tiles\, rendered}{Number\, of\, tiles}}
\label{FrameProgr}
\end{equation}
\begin{equation}
\small{ExpProgress = \frac{Time\, elapsed\, in\, current\, frame}{Target\, frame\, time}}
\label{expProgress}
\end{equation}

\begin{figure}[H]
	\centering
	\includegraphics[width = 9 cm]{graphics/QoS-algorithm.PNG}
	\caption{Dynamic priority algorithm for QoS \cite{QoS} \label{QoS}}
\end{figure}
For implementing the algorithm, a simple policy is used, described in Figure \ref{QoS}. The algorithm evaluates the current progress in the current frame of the GPU and also the expected progress (equations \ref{FrameProgr} and \ref{expProgress} from \cite{QoS}), and, based on these results, the memory controller decides the needed priority levels.  

\section{Comparisons}
A more detailed view of the performance improvement of the three separate solutions is presented in the sections below:

\subsection{Cache management in shared LLC}
A comparison to other cache management policies was made with 3 different configurations: 1, 2 and 4 CPU cores, each with 4 GPU cores (properly named 1C4G, 2C4G and 4C4G respectively). A different random benchmark on each CPU core was run, and the same random benchmark for all 4 GPUs. For each configuration, the benchmark was changed 100, 40 and 30 times respectively. The reduction of cache misses and speedup was calculated relative to LRU results. Figure~\ref{fig:comp} shows the comparison.

\begin{figure}[H]
	\centering
	\includegraphics[width = 8 cm]{graphics/LLCresults.png}
	\caption{Percentage reduction in misses and percentage speedup in the CPUs and the GPUs caused by HeLM and other cache management policies in comparison to LRU \cite{LLC}}\label{fig:comp}
\end{figure} 

As can be seen in Figure~\ref{fig:comp}, HeLM offer a considerably higher reduction in cache misses and overall speedup for the CPUs. For the 1C4G configuration, the speedup amounted to 29.5\% compared to LRU, 39.1\% for 2C4G and 33\% for 4C4G. On the GPUs, however, the effect is the opposite, although it is important to notice that a huge increase in misses for the GPU didn't affect so negatively the speedup. To provide a more general overview, the results for CPUs and GPUs were combined into Figure~\ref{fig:comp1}. \cite{LLC}

\begin{figure}[H]
	\centering
	\includegraphics[width = 8 cm]{graphics/LLCresults2.png}
	\caption{Combined percentage speedup of HeLM and other cache management policies in comparison to LRU \cite{LLC}}\label{fig:comp1}
\end{figure} 

Figure~\ref{fig:comp1} shows the weighted average of the speedup for CPU and GPU workloads to obtain the overall speedup of HeLM. The improvement is very noticeable even though it caused a huge amount of misses on the GPU requests. This is because GPU programs are much less sensible to the cache than CPU programs. The speedup was 7.7\%, 10.4\%, and 12.5\% for 1C4G, 2C4G and 4C4G respectively. \cite{LLC}

\begin{figure}[H]
	\centering
	\includegraphics[width = 8 cm]{graphics/LLChardware.png}
	\caption{Hardware overhead of different cache management policies \cite{LLC}}\label{fig:LLChw}
\end{figure}

Finally, a comparison was made on the hardware overhead of HeLM and three other cache management policies. Figure~\ref{fig:LLChw} shows an explanation of the extra hardware needed for each cache policy and a count of the number of flip-flops needed to implement it. As can be seen, the policy with the least overhead is TAP, however it is not a big difference in comparison to HeLM. On the other hand, the speedup achieved by HeLM is considerably higher than TAP.

\subsection{Hybrid memory system}
Regarding the baseline in \cite{PDRAM}, although the improvements in energy consumption and memory size constraints do matter, a PRAM-only main memory system shows significant slowdowns for a regular SoC. The performance overhead is in average about 40\% making it not suitable for deployment in current computer architecture designs. 

For tackling the big overhead issue, the hybrid system has been proposed. 

The PRAM/DRAM main memory system significantly improves performance for the GPU, with small overhead on the CPU side. As presented in the experiments, the improvement over an uniform PRAM main memory is in average 24, using the DL+CM (dynamic list) memory management for the hot-write buffer. However, the reduction in CPU IPC can go up to 33\% for the SL+CM and 23.6\% with the DL+CM, taking in account a 8K entries (64B per entry) buffer\cite{PRAM}. 

Regarding the energy consumption constraints, the proposed solution in \cite{PDRAM} shows savings ranging from 12 to 50\% in comparison with the DRAM-only main memory baseline system. 

\subsection{Memory Scheduling}
The SMS is compared to three state-of-the-art memory scheduler configurations: FR-FCFS, ATLAS and TCM.  It is both compared having p = 0.9, i.e  90\% probability of using SJF, and with p = 0 where the batch scheduler always picks the round-robin policy.

For evaluation of the system a set of 105 multi programmed workloads are used each consisting of sixteen SPEC CPU 2006 benchmarks and one GPU application.  

To compare the performance of the different schedulers CPU-GPU Weighted Speedup (CGWS) metric (Eqn \ref{eq:CGWS})  is used.

\begin{equation}
CGWS = CPU_{WS} + GPU_{weight}*GPU_{speedup} \label{eq:CGWS}
\end{equation} 

Where
 
\begin{equation}
CPI_{WS} = \sum_{i=1}^{NumCores}\frac{IPC_i^{shared}}{IPC_i^{alone}}
\end{equation}

And 

\begin{equation}
GPU_{Speedup} = \frac{GPU_{FramRate}^{shared}}{GPU_{FramRate}^{alone}}
\end{equation}

The CPU benchmarks are classified into three categories based on their memory intensity measured as last level cache misses per thousand instructions (MPKI): LOW, MEDIUM and HIGH where L $<$ 1 $<$ M $<$ 15 $<$ H MPKI.

Based on these three categories random benchmarks are picked from each category to make seven different intensity mixes: L (ALL Low), ML (Medium/Low), M (All Medium), HL (High/Low),  HML (High/Medium/Low),  HM (High/Medium) and H (All High).  A co-running GPU benchmark is randomly selected for each workload.

In systems where CPU performance is of more importance than GPU SMS$_{0.9}$ can significantly improve all over system performance while SMS$_0$ reduces performance. As can be seen in Figure \ref{fig:SmS_GPU_1} SMS$_{0.9}$ provides 6.4\%/17.2\%/25.7\% system performance improvement and 244.6\%/47.6\%/205.7\% fairness improvement over FR-FCFS/ATLAS/TCM.
But SMS$_0$ provides 6.8\%/33.4\%/24.3\% system performance degradation and 15.2\%/89.7\%/36.7\% fairness degradation over FR-FCFS/ATLAS/TCM, respectively \cite{SmS}

\begin{figure}[H]
	\centering
	\includegraphics[width = 9.2 cm]{graphics/GPU_W_1.png}
	\caption{System performance and fairness for 7 workload categories (105 workloads) \cite{SmS}}\label{fig:SmS_GPU_1}
\end{figure}

In systems where GPU performance matters more to the user, e.g gaming, SMS$_0$ provides an all over better performance.  As can be seen in Figure \ref{fig:SmS_GPU_1000}  SMS$_0$ provides 1.6\%/32.7\%/16.4\% system performance improvement, while SMS$_{0.9}$ provides 44.3\%/13.3\%/27.7\% system performance degradation, over FR-FCFS/ATLAS/TCM

\begin{figure}[H]
	\centering
	\includegraphics[width = 9 cm]{graphics/GPU_W_1000.png}
	\caption{System performance for 7 workload categories (105 workloads) for SMS$_0$ \cite{SmS}}\label{fig:SmS_GPU_1000}
\end{figure}

\subsection{Dynamic QoS}
The presented algorithm was tested against a static (static CPU priority over GPU) QoS policy and an uncontrolled bandwidth. The use of the static QoS policy showed performance degradation ranging from 33 to 50\%, resulting in more dropped frames for the GPU, also with severe impact on the CPU IPC. The reason is that a lower number of requests for the GPU reduces the scheduling efficiency, resulting, in some cases in lower performance than the uncontrolled bandwidth version.

Using the dynamic Quality of Service policy with 32 outstanding requests from the GPU, the results of the experiment presented in paper \cite{QoS} show modest degradation in CPU performance with slowdowns ranging from 1.02 to 1.15 comparing with the CPU-only SoC. However, in all the tested benchmarks, the integrated graphics card managed to reduce the number of dropped frames, resulting in an acceptable number of Frames Per Second (30 FPS is considered a minimum acceptable value). The results can be seen in Figure \ref{QoSFPS}.

\begin{figure}[H]
	\centering
	\includegraphics[width = 9 cm]{graphics/QoSfps.PNG}
	\caption{GPU performance in frames per second when the CPU is under heavy load (art)\cite{QoS}}\label{QoSFPS}
\end{figure}

\subsection{Hardware overhead of the proposed solutions}
For the implementation of HeLM as an LLC cache management policy, several counters and registered are needed. First of all, to identify the cache sensitivity of the GPU application an instruction counter per core is needed (20 bits x 4), as well as some core IDs (10-bit x 4). To count and compare the CPU cache misses with each of the two thresholds, two new counters are also needed (20-bit x 2). Finally, a register is used to hold the LLC bypass threshold selected by the management policy (6 bits). In total, this solution inserts an overhead of 166 bits.

In the case of the hybrid DRAM/PRAM solution, the overhead is represented by a write address list which is implemented as a cache. Based on the empirical information gathered by the researchers, the address list ranges from 1K to 8K entries. Each entry contains a DRAM and a PRAM address, amounting to 79 bits. This means that the area can fluctuate from a minimum of 9KB (approximately 1K*79b) to a maximum of 79KB (= 8K*79b) - last one chosen as the best of option with fairly good improvements in GPU performance.

Finally, the SMS memory scheduler hardware overhead can be calculated using the formulas given in Table~\ref{tab:SmS1}, Table~\ref{tab:SmS2} and Table~\ref{tab:SmS3}. The data shown in the tables was calculated for 16 CPU cores and 1 GPU with 20 cores. For comparison, the analysis will be based on 1 GPU with 4 cores and 4 CPUs such as one of the configurations of the HeLM solution. For the GPU calculations, the result will be the same, as the resources are needed for the whole GPU independent of cores. So for example we would still need the same 20 entries for the GPU FIFO queues. For the CPU we can use a rule of three and it can be calculated that 40 entries are needed for the CPU FIFO queues. because they are required for the whole GPU not per core. The MPKC counters will have 40 bits, and the last requests row index another 60 bits. For Stage 2, the CPU memory request counters will represent 20 bits, while the GPU ones another 10 bits. Finally for Stage 3, the PerBank FIFO queues will have 120 entries supposing we have the same DRAM memory. In total this solution would require 130 bit counters and 3 queues with 40, 20 and 120 entries.

\section{Analysis}
The issues that have arose with the integration of GPU on the same die with the CPU need tackling on different fronts in order to take benefit of the increase in raw computing power that a heterogeneous SoC exhibits. 
The LLC management technique, the staged memory scheduler and the dynamic QoS are sequential improvements that can be made integrated into a heterogeneous SoC without changing the overall system architecture. Therefore, the previously mentioned solutions can be integrated in the design of current processing systems. The outcome of the integration can show improvements on three important steps in the data access blocks:
\begin{itemize}
	\item
	Increase in the LLC cache hit ratio for the GPU.
	\item
	Better instruction fetch by reorganizing the conventional request buffer.
	\item
	Improved end-user experience by means of enriched GPU rendering quality with minimal impact on CPU performance.
\end{itemize}

The hybrid memory system comes to complement the two previously mentioned solution in terms of system scaling, i.e. where such a bound constrains performance.

To be able to compare the three solutions, two cases were analyzed. The first case consists on fields where small area and low power consumption are a priority, such as embedded systems for Internet of Things and similar applications. Given such requirements, simpler and more efficient solutions are preferred over high performance ones.

Of the three papers analyzed, the HeLM cache management policy requires the least hardware overhead, just 166 bits, and thus the least chip and board area increase, while still offering a considerable speedup of at least 7.7\% up to 12.5\% compared to LRU, the simplest other solution. In terms of energy consumption, it could be argued that bypassing the LLC would cause an increase in accesses to the off-chip memory, however much of the bypassing is done whenever the application being run is cache insensitive. In that case most LLC accesses will miss and the main memory will still have to be used anyway. This means that energy-wise, a slight benefit could be seen by not having to go through the LLC.

On the other hand, the PRAM solution would require the addition of another memory module to the board which means bigger form factor and higher costs. Not just that, but the need for having an extra cache on-chip, with a more considerable size of 79 kilobytes, means that this is the most cumbersome proposal. However, PRAM is a non-volatile memory, meaning that it can be completely turned off with zero power consumption for the deepest sleep modes. This is a very powerful and useful ability for very low consumption systems.

The memory scheduler solution can be considered a middle point between the two previous onces. Its speedup is at least 6.4\% but can be as much as 25.7\% compared to other state-of-the-art alternatives. Its overhead is mostly on on-chip area, and consists of 130 bits and 3 different queues of 40, 20 and 120 entries. Compared to HeLM, the difference in overhead is big, but still with a smaller footprint than PRAM.

The second case that was analyzed consisted on having the best performance regardless of other factors, like area and power consumption. In this case, the memory scheduling solution would be the most appropriate since its speedup is the biggest of the three. The HeLM cache management policy could also be a good offer, but it doesn't perform as good and it wouldn't be advisable to have both of them since they mostly overlap and the improvement could be negligible. PRAM would simply not be a good solution since it offers a worse performance than just using DRAM. However it would be very desirable in systems with a big need for reduced power consumption and a big amount of main memory.

For the sake of keeping an adequate user-experience when using the GPU for graphics rendering, the dynamic QoS policy can be thrown into play. This is the case of mobile game consoles, where a bandwidth-bounded GPU can dramatically reduce the device's overall usability. Integrating the mechanism into the memory controller favors priority for the CPU, while keeping an acceptable level of frame rate for the GPU.

\section{Conclusion}
Recent developments in the field of main memory shows a broad in horizons, as researchers try to evaluate several different paths and approaches towards paving the way for another decade of sustained scaling. 

In this paper, two different approaches have been studied for the evaluation of memory systems roadmap: technology-aware improvements (the combination of two technologies that complement each other)  and application-aware solutions (better management of memory resources). 

Because of the challenges faced when combining two different execution units (the CPU and the integrated GPU - GPU) on the same die, the focus has to be shifted to multiple aspects and critical points, such as bandwidth, latency, capacity, reliability and energy consumption. Therefore, it is not enough to balance the load between the two processors, but to dynamically prioritize memory access \cite{LLC} and bandwidth allocation in order to adapt to different types of applications and to improve the overall user experience. 

Aside from application-aware memory allocation, the attention needs to be drawn also to the efficient management of the on-chip request buffer in order to allow a appropiate ratio of instruction fetch for both compute cores, without a significant degradation in performance for neither of them. The  staged memory scheduler proves a good solution for the presented issue without a considerable impact on die area, costs or complexity.  It shows to improve performance and fairness compared to state-of-the-art memory schedulers and is significantly easer to implement\cite{SmS}.

Regarding the area and energy consumption constraints, the hybrid main-memory organization presented in \cite{PRAM} and \cite{PDRAM} can help extend the roadmap of the current computer architectures, facilitating the integration in low-power devices. The synergy of the two technologies, combined with the dynamic hot-write buffer and management unit have a very small impact on CPU performance, greatly improving GPU IPC. Therefore, as stated in \cite{PRAM}, the increase in GPU computer power can be leveraged to build more powerful SoCs and make more high-performance compact devices.

% use section* for acknowledgement
\ifCLASSOPTIONcompsoc

% Can use something like this to put references on a page
% by themselves when using endfloat and the captionsoff option.
\ifCLASSOPTIONcaptionsoff
  \newpage
\fi

\bibliographystyle{IEEEtran}

\bibliography{ref}

\newpage

% that's all folks
\end{document}
